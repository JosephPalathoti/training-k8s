Windows PowerShell
Copyright (C) Microsoft Corporation. All rights reserved.

Install the latest PowerShell for new features and improvements! https://aka.ms/PSWindows

PS C:\Users\vijay> ssh sfjbs@20.244.32.239
The authenticity of host '20.244.32.239 (20.244.32.239)' can't be established.
ED25519 key fingerprint is SHA256:92/8TKhnPMWPW6/GR/zRwes8U2dvAbEeJTTXAABlcgM.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '20.244.32.239' (ED25519) to the list of known hosts.
sfjbs@20.244.32.239's password:
Welcome to Ubuntu 20.04.5 LTS (GNU/Linux 5.15.0-1022-azure x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Wed Nov  2 06:09:40 UTC 2022

  System load:  0.0               Processes:              128
  Usage of /:   9.3% of 28.89GB   Users logged in:        0
  Memory usage: 5%                IPv4 address for eth0:  10.0.0.5
  Swap usage:   0%                IPv4 address for weave: 10.32.0.1

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

10 updates can be applied immediately.
To see these additional updates run: apt list --upgradable


*** System restart required ***
Last login: Mon Oct 31 03:30:01 2022 from 103.149.126.203
sfjbs@vm002:~$
sfjbs@vm002:~$ nproc
2
sfjbs@vm002:~$ free -h
              total        used        free      shared  buff/cache   available
Mem:          7.8Gi       370Mi       5.4Gi       4.0Mi       2.0Gi       7.2Gi
Swap:            0B          0B          0B
sfjbs@vm002:~$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/root        29G  2.7G   27G  10% /
devtmpfs        3.9G     0  3.9G   0% /dev
tmpfs           3.9G     0  3.9G   0% /dev/shm
Preparing to unpack .../10-snapd_2.57.5+20.04_amd64.deb ...
Unpacking snapd (2.57.5+20.04) over (2.55.5+20.04) ...
Preparing to unpack .../11-sosreport_4.4-1ubuntu0.20.04.1_amd64.deb ...
Unpacking sosreport (4.4-1ubuntu0.20.04.1) over (4.3-1ubuntu0.20.04.2) ...
Setting up snapd (2.57.5+20.04) ...
Created symlink /etc/systemd/system/multi-user.target.wants/snapd.aa-prompt-listener.service â†’ /lib/systemd/system/snapd.aa-prompt-listener.service.
snapd.failure.service is a disabled or a static unit not running, not starting it.
snapd.snap-repair.service is a disabled or a static unit not running, not starting it.
Setting up libxmlb2:amd64 (0.3.6-2build1~20.04.1) ...
Setting up libfwupd2:amd64 (1.7.9-1~20.04.1) ...
Setting up tzdata (2022e-0ubuntu0.20.04.0) ...

Current default time zone: 'Etc/UTC'
Local time is now:      Wed Nov  2 06:21:13 UTC 2022.
Universal Time is now:  Wed Nov  2 06:21:13 UTC 2022.
Run 'dpkg-reconfigure tzdata' if you wish to change it.

Setting up kubectl (1.25.3-00) ...
Setting up sosreport (4.4-1ubuntu0.20.04.1) ...
Setting up kubelet (1.25.3-00) ...
Setting up grub-efi-amd64-bin (2.04-1ubuntu47.4) ...
Setting up libfwupdplugin5:amd64 (1.7.9-1~20.04.1) ...
Setting up fwupd (1.7.9-1~20.04.1) ...
Installing new version of config file /etc/fwupd/redfish.conf ...
fwupd-offline-update.service is a disabled or a static unit not running, not starting it.
fwupd-refresh.service is a disabled or a static unit not running, not starting it.
fwupd.service is a disabled or a static unit not running, not starting it.
Setting up kubeadm (1.25.3-00) ...
Setting up grub-efi-amd64-signed (1.173.2~20.04.1+2.04-1ubuntu47.4) ...
Trying to migrate /boot/efi into esp config
Installing grub to /boot/efi.
Installing for x86_64-efi platform.
Installation finished. No error reported.
Processing triggers for man-db (2.9.1-1) ...
Processing triggers for dbus (1.12.16-2ubuntu2.3) ...
Processing triggers for mime-support (3.64ubuntu1) ...
Processing triggers for libc-bin (2.31-0ubuntu9.9) ...
sfjbs@vm002:~$ sudo reboot
Connection to 20.244.32.239 closed by remote host.
Connection to 20.244.32.239 closed.
PS C:\Users\vijay> ssh sfjbs@20.244.32.239
ssh: connect to host 20.244.32.239 port 22: Connection timed out
PS C:\Users\vijay> ssh sfjbs@20.244.32.239
ssh: connect to host 20.244.32.239 port 22: Connection timed out
PS C:\Users\vijay> ssh sfjbs@20.244.32.239
sfjbs@20.244.32.239's password:
Welcome to Ubuntu 20.04.5 LTS (GNU/Linux 5.15.0-1022-azure x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Wed Nov  2 06:23:41 UTC 2022

  System load:  0.82              Processes:             136
  Usage of /:   9.6% of 28.89GB   Users logged in:       0
  Memory usage: 4%                IPv4 address for eth0: 10.0.0.5
  Swap usage:   0%

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

0 updates can be applied immediately.

New release '22.04.1 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Wed Nov  2 06:09:41 2022 from 103.208.71.153
sfjbs@vm002:~$ sudo apt-get install containerd -y
Get:5 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2196 kB]
Get:6 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 c-n-f Metadata [16.0 kB]
Get:7 http://azure.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [972 kB]
Get:8 http://azure.archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [21.8 kB]
Get:10 http://azure.archive.ubuntu.com/ubuntu focal-security/main amd64 Packages [1821 kB]
Get:11 http://azure.archive.ubuntu.com/ubuntu focal-security/main amd64 c-n-f Metadata [11.2 kB]
Get:12 http://azure.archive.ubuntu.com/ubuntu focal-security/universe amd64 Packages [743 kB]
Get:13 http://azure.archive.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [15.3 kB]
Hit:9 https://packages.cloud.google.com/apt kubernetes-xenial InRelease
Fetched 6133 kB in 1s (5249 kB/s)
Reading package lists... Done
sfjbs@vm002:~$ sudo apt install kubelet=1.24.7-00 kubeadm=1.24.7-00 kubectl=1.24.7-00  --allow-downgrades -y
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages will be DOWNGRADED:
  kubeadm kubectl kubelet
0 upgraded, 0 newly installed, 3 downgraded, 0 to remove and 0 not upgraded.
Need to get 37.5 MB of archives.
After this operation, 3156 kB of additional disk space will be used.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.24.7-00 [19.2 MB]
Get:2 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubectl amd64 1.24.7-00 [9326 kB]
Get:3 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.24.7-00 [9009 kB]
Fetched 37.5 MB in 6s (6356 kB/s)
dpkg: warning: downgrading kubelet from 1.25.3-00 to 1.24.7-00
(Reading database ... 58738 files and directories currently installed.)
Preparing to unpack .../kubelet_1.24.7-00_amd64.deb ...
Unpacking kubelet (1.24.7-00) over (1.25.3-00) ...
dpkg: warning: downgrading kubectl from 1.25.3-00 to 1.24.7-00
Preparing to unpack .../kubectl_1.24.7-00_amd64.deb ...
Unpacking kubectl (1.24.7-00) over (1.25.3-00) ...
dpkg: warning: downgrading kubeadm from 1.25.3-00 to 1.24.7-00
Preparing to unpack .../kubeadm_1.24.7-00_amd64.deb ...
Unpacking kubeadm (1.24.7-00) over (1.25.3-00) ...
Setting up kubectl (1.24.7-00) ...
Setting up kubelet (1.24.7-00) ...
Setting up kubeadm (1.24.7-00) ...
sfjbs@vm002:~$ sudo modprobe overlay
netfiltersfjbs@vm002:~$ sudo modprobe br_netfilter
sfjbs@vm002:~$ sudo sysctl --system
* Applying /etc/sysctl.d/10-console-messages.conf ...
kernel.printk = 4 4 1 7
* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...
net.ipv6.conf.all.use_tempaddr = 2
net.ipv6.conf.default.use_tempaddr = 2
* Applying /etc/sysctl.d/10-kernel-hardening.conf ...
kernel.kptr_restrict = 1
* Applying /etc/sysctl.d/10-link-restrictions.conf ...
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/10-magic-sysrq.conf ...
kernel.sysrq = 176
* Applying /etc/sysctl.d/10-network-security.conf ...
net.ipv4.conf.default.rp_filter = 2
net.ipv4.conf.all.rp_filter = 2
* Applying /etc/sysctl.d/10-ptrace.conf ...
kernel.yama.ptrace_scope = 1
* Applying /etc/sysctl.d/10-zeropage.conf ...
vm.mmap_min_addr = 65536
* Applying /usr/lib/sysctl.d/50-default.conf ...
net.ipv4.conf.default.promote_secondaries = 1
sysctl: setting key "net.ipv4.conf.all.promote_secondaries": Invalid argument
net.ipv4.ping_group_range = 0 2147483647
net.core.default_qdisc = fq_codel
fs.protected_regular = 1
fs.protected_fifos = 1
* Applying /usr/lib/sysctl.d/50-pid-max.conf ...
kernel.pid_max = 4194304
* Applying /etc/sysctl.d/99-cloudimg-ipv6.conf ...
net.ipv6.conf.all.use_tempaddr = 0
net.ipv6.conf.default.use_tempaddr = 0
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/kubernetes.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
* Applying /usr/lib/sysctl.d/protect-links.conf ...
fs.protected_fifos = 1
fs.protected_hardlinks = 1
fs.protected_regular = 2
fs.protected_symlinks = 1
* Applying /etc/sysctl.conf ...
sfjbs@vm002:~$
sfjbs@vm002:~$ sudo systemctl remove containerd
Unknown operation remove.
sfjbs@vm002:~$ sudo apt remove containerd -y
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following package was automatically installed and is no longer required:
  runc
Use 'sudo apt autoremove' to remove it.
The following packages will be REMOVED:
  containerd
0 upgraded, 0 newly installed, 1 to remove and 3 not upgraded.
After this operation, 150 MB disk space will be freed.
(Reading database ... 58738 files and directories currently installed.)
Removing containerd (1.5.9-0ubuntu1~20.04.4) ...
Processing triggers for man-db (2.9.1-1) ...
sfjbs@vm002:~$ sudo apt-get install containerd -y
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following NEW packages will be installed:
  containerd
0 upgraded, 1 newly installed, 0 to remove and 3 not upgraded.
Need to get 0 B/33.0 MB of archives.
After this operation, 150 MB of additional disk space will be used.
Selecting previously unselected package containerd.
(Reading database ... 58709 files and directories currently installed.)
Preparing to unpack .../containerd_1.5.9-0ubuntu1~20.04.4_amd64.deb ...
Unpacking containerd (1.5.9-0ubuntu1~20.04.4) ...
Setting up containerd (1.5.9-0ubuntu1~20.04.4) ...
Processing triggers for man-db (2.9.1-1) ...
sfjbs@vm002:~$
sfjbs@vm002:~$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --service-cidr=192.168.0.0/24  --apiserver-cert-extra-sans=20.244.32.239
I1102 07:43:15.045901    6336 version.go:255] remote version is much newer: v1.25.3; falling back to: stable-1.24
[init] Using Kubernetes version: v1.24.7
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local vm002] and IPs [192.168.0.1 10.0.0.5 20.244.32.239]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost vm002] and IPs [10.0.0.5 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost vm002] and IPs [10.0.0.5 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 17.003458 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node vm002 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node vm002 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: r492kl.zr1dwzvjsoad1b6w
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.0.5:6443 --token r492kl.zr1dwzvjsoad1b6w \
        --discovery-token-ca-cert-hash sha256:4f9e1fc68085e08bc3fe63ef8180bd8ade275e38fff2aec6c1f3209e737b3df2
sfjbs@vm002:~$   mkdir -p $HOME/.kube
sfjbs@vm002:~$   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sfjbs@vm002:~$   sudo chown $(id -u):$(id -g) $HOME/.kube/config
sfjbs@vm002:~$ kubectl get node
NAME    STATUS   ROLES           AGE     VERSION
vm002   Ready    control-plane   3m13s   v1.24.7
sfjbs@vm002:~$ kubectl get po -A
NAMESPACE     NAME                            READY   STATUS              RESTARTS   AGE
kube-system   coredns-6d4b75cb6d-9xszf        0/1     ContainerCreating   0          3m16s
kube-system   coredns-6d4b75cb6d-h5p66        0/1     ContainerCreating   0          3m16s
kube-system   etcd-vm002                      1/1     Running             0          3m28s
kube-system   kube-apiserver-vm002            1/1     Running             0          3m28s
kube-system   kube-controller-manager-vm002   1/1     Running             0          3m28s
kube-system   kube-proxy-ftn67                1/1     Running             0          3m16s
kube-system   kube-scheduler-vm002            1/1     Running             0          3m28s
sfjbs@vm002:~$ sudo cd /etc/kubernetes
sudo: cd: command not found
sfjbs@vm002:~$ ^Cd /etc/kubernetes
sfjbs@vm002:~$ sudo bash
root@vm002:/home/sfjbs# cd /etc/kubernetes/
root@vm002:/etc/kubernetes# ls
admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf
root@vm002:/etc/kubernetes# cd manifests/
root@vm002:/etc/kubernetes/manifests# ls
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
root@vm002:/etc/kubernetes/manifests# cd ..
root@vm002:/etc/kubernetes# ls
admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf
root@vm002:/etc/kubernetes# vim kubelet.conf
root@vm002:/etc/kubernetes# ls
admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf
root@vm002:/etc/kubernetes# containerd
containerd               containerd-shim          containerd-shim-runc-v1  containerd-shim-runc-v2  containerd-stress
root@vm002:/etc/kubernetes# containerd --version
containerd github.com/containerd/containerd 1.5.9-0ubuntu1~20.04.4
root@vm002:/etc/kubernetes#
root@vm002:/etc/kubernetes# containerd
containerd               containerd-shim          containerd-shim-runc-v1  containerd-shim-runc-v2  containerd-stress
root@vm002:/etc/kubernetes# containerd^C
root@vm002:/etc/kubernetes# exit
sfjbs@vm002:~$
sfjbs@vm002:~$ kubectl get node
NAME    STATUS   ROLES           AGE   VERSION
vm001   Ready    <none>          62s   v1.24.7
vm002   Ready    control-plane   12m   v1.24.7
sfjbs@vm002:~$ kubectl get po -A
NAMESPACE     NAME                            READY   STATUS              RESTARTS   AGE
kube-system   coredns-6d4b75cb6d-9xszf        0/1     ContainerCreating   0          12m
kube-system   coredns-6d4b75cb6d-h5p66        0/1     ContainerCreating   0          12m
kube-system   etcd-vm002                      1/1     Running             0          13m
kube-system   kube-apiserver-vm002            1/1     Running             0          13m
kube-system   kube-controller-manager-vm002   1/1     Running             0          13m
kube-system   kube-proxy-ftn67                1/1     Running             0          12m
kube-system   kube-proxy-wssm6                1/1     Running             0          82s
kube-system   kube-scheduler-vm002            1/1     Running             0          13m
sfjbs@vm002:~$ kubectl get po -A -owide
NAMESPACE     NAME                            READY   STATUS              RESTARTS   AGE     IP         NODE    NOMINATED NODE   READINESS GATES
kube-system   coredns-6d4b75cb6d-9xszf        0/1     ContainerCreating   0          14m     <none>     vm002   <none>           <none>
kube-system   coredns-6d4b75cb6d-h5p66        0/1     ContainerCreating   0          14m     <none>     vm002   <none>           <none>
kube-system   etcd-vm002                      1/1     Running             0          14m     10.0.0.5   vm002   <none>           <none>
kube-system   kube-apiserver-vm002            1/1     Running             0          14m     10.0.0.5   vm002   <none>           <none>
kube-system   kube-controller-manager-vm002   1/1     Running             0          14m     10.0.0.5   vm002   <none>           <none>
kube-system   kube-proxy-ftn67                1/1     Running             0          14m     10.0.0.5   vm002   <none>           <none>
kube-system   kube-proxy-wssm6                1/1     Running             0          2m59s   10.0.0.4   vm001   <none>           <none>
kube-system   kube-scheduler-vm002            1/1     Running             0          14m     10.0.0.5   vm002   <none>           <none>
sfjbs@vm002:~$ kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
serviceaccount/weave-net created
clusterrole.rbac.authorization.k8s.io/weave-net created
clusterrolebinding.rbac.authorization.k8s.io/weave-net created
role.rbac.authorization.k8s.io/weave-net created
rolebinding.rbac.authorization.k8s.io/weave-net created
daemonset.apps/weave-net created
sfjbs@vm002:~$ kubectl get po -A -owide
NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE     IP          NODE    NOMINATED NODE   READINESS GATES
kube-system   coredns-6d4b75cb6d-9xszf        1/1     Running   0          16m     10.44.0.1   vm002   <none>           <none>
kube-system   coredns-6d4b75cb6d-h5p66        1/1     Running   0          16m     10.44.0.2   vm002   <none>           <none>
kube-system   etcd-vm002                      1/1     Running   0          17m     10.0.0.5    vm002   <none>           <none>
kube-system   kube-apiserver-vm002            1/1     Running   0          17m     10.0.0.5    vm002   <none>           <none>
kube-system   kube-controller-manager-vm002   1/1     Running   0          17m     10.0.0.5    vm002   <none>           <none>
kube-system   kube-proxy-ftn67                1/1     Running   0          16m     10.0.0.5    vm002   <none>           <none>
kube-system   kube-proxy-wssm6                1/1     Running   0          5m18s   10.0.0.4    vm001   <none>           <none>
kube-system   kube-scheduler-vm002            1/1     Running   0          17m     10.0.0.5    vm002   <none>           <none>
kube-system   weave-net-5hr7s                 2/2     Running   0          35s     10.0.0.4    vm001   <none>           <none>
kube-system   weave-net-xdpjb                 2/2     Running   0          35s     10.0.0.5    vm002   <none>           <none>
sfjbs@vm002:~$ kubectl create deploy testapp --image=nginx:latest
deployment.apps/testapp created
sfjbs@vm002:~$ kubectl get po
NAME                       READY   STATUS              RESTARTS   AGE
testapp-5c57b6c764-dlsrw   0/1     ContainerCreating   0          4s
sfjbs@vm002:~$ kubectl get po
NAME                       READY   STATUS              RESTARTS   AGE
testapp-5c57b6c764-dlsrw   0/1     ContainerCreating   0          6s
sfjbs@vm002:~$ kubectl get po
NAME                       READY   STATUS              RESTARTS   AGE
testapp-5c57b6c764-dlsrw   0/1     ContainerCreating   0          6s
sfjbs@vm002:~$ kubectl get po
NAME                       READY   STATUS              RESTARTS   AGE
testapp-5c57b6c764-dlsrw   0/1     ContainerCreating   0          7s
sfjbs@vm002:~$ kubectl get po
NAME                       READY   STATUS              RESTARTS   AGE
testapp-5c57b6c764-dlsrw   0/1     ContainerCreating   0          7s
sfjbs@vm002:~$ watch kubectl get po
sfjbs@vm002:~$ kubectl get po -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP          NODE    NOMINATED NODE   READINESS GATES
testapp-5c57b6c764-dlsrw   1/1     Running   0          32s   10.32.0.2   vm001   <none>           <none>
sfjbs@vm002:~$ sudo bash
root@vm002:/home/sfjbs# cd /etc/apt/sources.list.d/
root@vm002:/etc/apt/sources.list.d# ls
root@vm002:/etc/apt/sources.list.d# exit
sfjbs@vm002:~$ cd
sfjbs@vm002:~$
sfjbs@vm002:~$ kubectl get node
NAME    STATUS   ROLES           AGE   VERSION
vm001   Ready    <none>          14m   v1.24.7
vm002   Ready    control-plane   26m   v1.24.7
sfjbs@vm002:~$ kubectl apply https://raw.githubusercontent.com/sharmavijay86/sharmavijay86.github.io/master/blog/k8ssetup/components.yaml
error: must specify one of -f and -k
sfjbs@vm002:~$ kubectl apply -f https://raw.githubusercontent.com/sharmavijay86/sharmavijay86.github.io/master/blog/k8ssetup/components.yaml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
sfjbs@vm002:~$ kubectl get po -A
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE
default       testapp-5c57b6c764-dlsrw          1/1     Running   0          9m7s
kube-system   coredns-6d4b75cb6d-9xszf          1/1     Running   0          27m
kube-system   coredns-6d4b75cb6d-h5p66          1/1     Running   0          27m
kube-system   etcd-vm002                        1/1     Running   0          27m
kube-system   kube-apiserver-vm002              1/1     Running   0          27m
kube-system   kube-controller-manager-vm002     1/1     Running   0          27m
kube-system   kube-proxy-ftn67                  1/1     Running   0          27m
kube-system   kube-proxy-wssm6                  1/1     Running   0          15m
kube-system   kube-scheduler-vm002              1/1     Running   0          27m
kube-system   metrics-server-7fbb5f69c6-hg22j   0/1     Running   0          16s
kube-system   weave-net-5hr7s                   2/2     Running   0          10m
kube-system   weave-net-xdpjb                   2/2     Running   0          10m
sfjbs@vm002:~$ watch kubectl get po -A
sfjbs@vm002:~$ kubectl logs  metrics-server-7fbb5f69c6-hg22j -n kube-system
I1102 08:11:20.687836       1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)
I1102 08:11:21.094535       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I1102 08:11:21.094660       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
I1102 08:11:21.094750       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1102 08:11:21.094813       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1102 08:11:21.094892       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1102 08:11:21.094940       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1102 08:11:21.095287       1 secure_serving.go:266] Serving securely on [::]:4443
I1102 08:11:21.095476       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key"
I1102 08:11:21.095887       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1102 08:11:21.096093       1 shared_informer.go:372] The sharedIndexInformer has started, run more than once is not allowed
I1102 08:11:21.195750       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1102 08:11:21.195769       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController
I1102 08:11:21.195814       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
sfjbs@vm002:~$ kubectl logs  metrics-server-7fbb5f69c6-hg22j -n kube-system -f
I1102 08:11:20.687836       1 serving.go:342] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)
I1102 08:11:21.094535       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I1102 08:11:21.094660       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
I1102 08:11:21.094750       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1102 08:11:21.094813       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1102 08:11:21.094892       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1102 08:11:21.094940       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1102 08:11:21.095287       1 secure_serving.go:266] Serving securely on [::]:4443
I1102 08:11:21.095476       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key"
I1102 08:11:21.095887       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1102 08:11:21.096093       1 shared_informer.go:372] The sharedIndexInformer has started, run more than once is not allowed
I1102 08:11:21.195750       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1102 08:11:21.195769       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController
I1102 08:11:21.195814       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
^C
sfjbs@vm002:~$ kubectl get po -A
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE
default       testapp-5c57b6c764-dlsrw          1/1     Running   0          9m54s
kube-system   coredns-6d4b75cb6d-9xszf          1/1     Running   0          27m
kube-system   coredns-6d4b75cb6d-h5p66          1/1     Running   0          27m
kube-system   etcd-vm002                        1/1     Running   0          28m
kube-system   kube-apiserver-vm002              1/1     Running   0          28m
kube-system   kube-controller-manager-vm002     1/1     Running   0          28m
kube-system   kube-proxy-ftn67                  1/1     Running   0          27m
kube-system   kube-proxy-wssm6                  1/1     Running   0          16m
kube-system   kube-scheduler-vm002              1/1     Running   0          28m
kube-system   metrics-server-7fbb5f69c6-hg22j   1/1     Running   0          63s
kube-system   weave-net-5hr7s                   2/2     Running   0          11m
kube-system   weave-net-xdpjb                   2/2     Running   0          11m
sfjbs@vm002:~$ kubectl top node
NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
vm001   30m          1%     872Mi           11%
vm002   103m         5%     1322Mi          16%
sfjbs@vm002:~$ kubectl top node
NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
vm001   30m          1%     870Mi           11%
vm002   94m          4%     1308Mi          16%
sfjbs@vm002:~$ kubectl top node
NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
vm001   30m          1%     870Mi           11%
vm002   94m          4%     1308Mi          16%
sfjbs@vm002:~$ kubectl top node
NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
vm001   30m          1%     870Mi           11%
vm002   94m          4%     1308Mi          16%
sfjbs@vm002:~$ client_loop: send disconnect: Connection reset
PS C:\Users\vijay>
